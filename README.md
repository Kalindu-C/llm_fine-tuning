ðŸ§  LLM Fine-Tuning
In this projects, I explored and fine-tuned several open-source Large Language Models (LLMs) to enhance performance on the target dataset.

ðŸ“Œ Models Used
Phi-2

Phi-3-mini

DeepSeek-R1-Distill-Llama-8B

Llama 3.2

ðŸ›  Libraries & Frameworks
unsloth/transformers â€“ optimized transformer library for faster fine-tuning and inference.

PyTorch â€“ core deep learning framework used for model training and evaluation.

ðŸ“‚ Dataset & Learning Outcomes
While working on this project, I:

Gained an in-depth understanding of LoRA and QLoRA fine-tuning techniques for efficient training.

Learned the tokenization process and how it impacts model performance.

Experimented with model parameter-efficient tuning to reduce computational cost while maintaining accuracy.
